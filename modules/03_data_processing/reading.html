<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Data Processing Reader</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="reading_files/libs/clipboard/clipboard.min.js"></script>
<script src="reading_files/libs/quarto-html/quarto.js"></script>
<script src="reading_files/libs/quarto-html/popper.min.js"></script>
<script src="reading_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="reading_files/libs/quarto-html/anchor.min.js"></script>
<link href="reading_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="reading_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="reading_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="reading_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="reading_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#why-data-processing-matters" id="toc-why-data-processing-matters" class="nav-link active" data-scroll-target="#why-data-processing-matters"><span class="header-section-number">1</span> Why Data Processing Matters</a>
  <ul class="collapse">
  <li><a href="#garbage-in-garbage-out" id="toc-garbage-in-garbage-out" class="nav-link" data-scroll-target="#garbage-in-garbage-out"><span class="header-section-number">1.1</span> “Garbage In, Garbage Out”</a></li>
  <li><a href="#data-are-not-neutral-objects" id="toc-data-are-not-neutral-objects" class="nav-link" data-scroll-target="#data-are-not-neutral-objects"><span class="header-section-number">1.2</span> Data Are Not Neutral Objects</a></li>
  <li><a href="#why-context-comes-before-cleaning" id="toc-why-context-comes-before-cleaning" class="nav-link" data-scroll-target="#why-context-comes-before-cleaning"><span class="header-section-number">1.3</span> Why Context Comes Before Cleaning</a></li>
  <li><a href="#data-processing-as-an-analytical-decision" id="toc-data-processing-as-an-analytical-decision" class="nav-link" data-scroll-target="#data-processing-as-an-analytical-decision"><span class="header-section-number">1.4</span> Data Processing as an Analytical Decision</a></li>
  </ul></li>
  <li><a href="#understanding-data-in-context" id="toc-understanding-data-in-context" class="nav-link" data-scroll-target="#understanding-data-in-context"><span class="header-section-number">2</span> Understanding Data in Context</a>
  <ul class="collapse">
  <li><a href="#how-data-are-collected-shapes-what-they-measure" id="toc-how-data-are-collected-shapes-what-they-measure" class="nav-link" data-scroll-target="#how-data-are-collected-shapes-what-they-measure"><span class="header-section-number">2.1</span> How Data Are Collected Shapes What They Measure</a></li>
  <li><a href="#example-mobile-device-location-data" id="toc-example-mobile-device-location-data" class="nav-link" data-scroll-target="#example-mobile-device-location-data"><span class="header-section-number">2.2</span> Example: Mobile Device Location Data</a></li>
  <li><a href="#implications-for-data-processing" id="toc-implications-for-data-processing" class="nav-link" data-scroll-target="#implications-for-data-processing"><span class="header-section-number">2.3</span> Implications for Data Processing</a></li>
  <li><a href="#context-informs-what-is-reasonable-to-change" id="toc-context-informs-what-is-reasonable-to-change" class="nav-link" data-scroll-target="#context-informs-what-is-reasonable-to-change"><span class="header-section-number">2.4</span> Context Informs What Is Reasonable to Change</a></li>
  <li><a href="#from-context-to-processing-decisions" id="toc-from-context-to-processing-decisions" class="nav-link" data-scroll-target="#from-context-to-processing-decisions"><span class="header-section-number">2.5</span> From Context to Processing Decisions</a></li>
  </ul></li>
  <li><a href="#what-do-we-mean-by-data-processing" id="toc-what-do-we-mean-by-data-processing" class="nav-link" data-scroll-target="#what-do-we-mean-by-data-processing"><span class="header-section-number">3</span> What Do We Mean by Data Processing?</a>
  <ul class="collapse">
  <li><a href="#beyond-cleaning" id="toc-beyond-cleaning" class="nav-link" data-scroll-target="#beyond-cleaning"><span class="header-section-number">3.1</span> Beyond “Cleaning”</a></li>
  <li><a href="#alignment-with-the-question" id="toc-alignment-with-the-question" class="nav-link" data-scroll-target="#alignment-with-the-question"><span class="header-section-number">3.2</span> Alignment With the Question</a></li>
  <li><a href="#common-components-of-data-processing" id="toc-common-components-of-data-processing" class="nav-link" data-scroll-target="#common-components-of-data-processing"><span class="header-section-number">3.3</span> Common Components of Data Processing</a></li>
  <li><a href="#processing-choices-can-affect-results" id="toc-processing-choices-can-affect-results" class="nav-link" data-scroll-target="#processing-choices-can-affect-results"><span class="header-section-number">3.4</span> Processing Choices Can Affect Results</a></li>
  <li><a href="#a-working-definition" id="toc-a-working-definition" class="nav-link" data-scroll-target="#a-working-definition"><span class="header-section-number">3.5</span> A Working Definition</a></li>
  </ul></li>
  <li><a href="#making-data-tidy" id="toc-making-data-tidy" class="nav-link" data-scroll-target="#making-data-tidy"><span class="header-section-number">4</span> Making Data Tidy</a>
  <ul class="collapse">
  <li><a href="#what-does-tidy-mean" id="toc-what-does-tidy-mean" class="nav-link" data-scroll-target="#what-does-tidy-mean"><span class="header-section-number">4.1</span> What Does “Tidy” Mean?</a></li>
  <li><a href="#a-common-untidy-example" id="toc-a-common-untidy-example" class="nav-link" data-scroll-target="#a-common-untidy-example"><span class="header-section-number">4.2</span> A Common Untidy Example</a></li>
  <li><a href="#the-tidy-version-of-the-same-data" id="toc-the-tidy-version-of-the-same-data" class="nav-link" data-scroll-target="#the-tidy-version-of-the-same-data"><span class="header-section-number">4.3</span> The Tidy Version of the Same Data</a></li>
  <li><a href="#when-structure-hides-meaning" id="toc-when-structure-hides-meaning" class="nav-link" data-scroll-target="#when-structure-hides-meaning"><span class="header-section-number">4.4</span> When Structure Hides Meaning</a></li>
  <li><a href="#why-tidy-data-matter-for-decision-making" id="toc-why-tidy-data-matter-for-decision-making" class="nav-link" data-scroll-target="#why-tidy-data-matter-for-decision-making"><span class="header-section-number">4.5</span> Why Tidy Data Matter for Decision-Making</a></li>
  <li><a href="#tidy-data-are-not-the-end-point" id="toc-tidy-data-are-not-the-end-point" class="nav-link" data-scroll-target="#tidy-data-are-not-the-end-point"><span class="header-section-number">4.6</span> Tidy Data Are Not the End Point</a></li>
  </ul></li>
  <li><a href="#inconsistent-records-and-duplicates" id="toc-inconsistent-records-and-duplicates" class="nav-link" data-scroll-target="#inconsistent-records-and-duplicates"><span class="header-section-number">5</span> Inconsistent Records and Duplicates</a>
  <ul class="collapse">
  <li><a href="#inconsistent-records" id="toc-inconsistent-records" class="nav-link" data-scroll-target="#inconsistent-records"><span class="header-section-number">5.1</span> Inconsistent Records</a></li>
  <li><a href="#why-inconsistencies-matter" id="toc-why-inconsistencies-matter" class="nav-link" data-scroll-target="#why-inconsistencies-matter"><span class="header-section-number">5.2</span> Why Inconsistencies Matter</a></li>
  <li><a href="#duplicate-records" id="toc-duplicate-records" class="nav-link" data-scroll-target="#duplicate-records"><span class="header-section-number">5.3</span> Duplicate Records</a></li>
  <li><a href="#resolving-inconsistencies-and-duplicates" id="toc-resolving-inconsistencies-and-duplicates" class="nav-link" data-scroll-target="#resolving-inconsistencies-and-duplicates"><span class="header-section-number">5.4</span> Resolving Inconsistencies and Duplicates</a></li>
  </ul></li>
  <li><a href="#encoding-and-variable-representation" id="toc-encoding-and-variable-representation" class="nav-link" data-scroll-target="#encoding-and-variable-representation"><span class="header-section-number">6</span> Encoding and Variable Representation</a>
  <ul class="collapse">
  <li><a href="#why-encoding-exists" id="toc-why-encoding-exists" class="nav-link" data-scroll-target="#why-encoding-exists"><span class="header-section-number">6.1</span> Why Encoding Exists</a></li>
  <li><a href="#when-encoding-causes-problems" id="toc-when-encoding-causes-problems" class="nav-link" data-scroll-target="#when-encoding-causes-problems"><span class="header-section-number">6.2</span> When Encoding Causes Problems</a></li>
  <li><a href="#inconsistent-or-ambiguous-encoding" id="toc-inconsistent-or-ambiguous-encoding" class="nav-link" data-scroll-target="#inconsistent-or-ambiguous-encoding"><span class="header-section-number">6.3</span> Inconsistent or Ambiguous Encoding</a></li>
  <li><a href="#encoding-and-interpretation" id="toc-encoding-and-interpretation" class="nav-link" data-scroll-target="#encoding-and-interpretation"><span class="header-section-number">6.4</span> Encoding and Interpretation</a></li>
  <li><a href="#encoding-as-a-processing-decision" id="toc-encoding-as-a-processing-decision" class="nav-link" data-scroll-target="#encoding-as-a-processing-decision"><span class="header-section-number">6.5</span> Encoding as a Processing Decision</a></li>
  </ul></li>
  <li><a href="#handling-missing-data-and-outliers" id="toc-handling-missing-data-and-outliers" class="nav-link" data-scroll-target="#handling-missing-data-and-outliers"><span class="header-section-number">7</span> Handling Missing Data and Outliers</a>
  <ul class="collapse">
  <li><a href="#why-data-are-missing" id="toc-why-data-are-missing" class="nav-link" data-scroll-target="#why-data-are-missing"><span class="header-section-number">7.1</span> Why Data Are Missing</a></li>
  <li><a href="#common-responses-to-missing-data" id="toc-common-responses-to-missing-data" class="nav-link" data-scroll-target="#common-responses-to-missing-data"><span class="header-section-number">7.2</span> Common Responses to Missing Data</a></li>
  <li><a href="#understanding-outliers" id="toc-understanding-outliers" class="nav-link" data-scroll-target="#understanding-outliers"><span class="header-section-number">7.3</span> Understanding Outliers</a></li>
  <li><a href="#outliers-and-analytical-goals" id="toc-outliers-and-analytical-goals" class="nav-link" data-scroll-target="#outliers-and-analytical-goals"><span class="header-section-number">7.4</span> Outliers and Analytical Goals</a></li>
  <li><a href="#transparency-and-documentation" id="toc-transparency-and-documentation" class="nav-link" data-scroll-target="#transparency-and-documentation"><span class="header-section-number">7.5</span> Transparency and Documentation</a></li>
  </ul></li>
  <li><a href="#aggregating-and-binning" id="toc-aggregating-and-binning" class="nav-link" data-scroll-target="#aggregating-and-binning"><span class="header-section-number">8</span> Aggregating and Binning</a>
  <ul class="collapse">
  <li><a href="#why-we-aggregate-and-bin-data" id="toc-why-we-aggregate-and-bin-data" class="nav-link" data-scroll-target="#why-we-aggregate-and-bin-data"><span class="header-section-number">8.1</span> Why We Aggregate and Bin Data</a></li>
  <li><a href="#what-is-lost-through-aggregation" id="toc-what-is-lost-through-aggregation" class="nav-link" data-scroll-target="#what-is-lost-through-aggregation"><span class="header-section-number">8.2</span> What Is Lost Through Aggregation</a></li>
  <li><a href="#binning-and-arbitrary-boundaries" id="toc-binning-and-arbitrary-boundaries" class="nav-link" data-scroll-target="#binning-and-arbitrary-boundaries"><span class="header-section-number">8.3</span> Binning and Arbitrary Boundaries</a></li>
  <li><a href="#aggregation-and-causal-interpretation" id="toc-aggregation-and-causal-interpretation" class="nav-link" data-scroll-target="#aggregation-and-causal-interpretation"><span class="header-section-number">8.4</span> Aggregation and Causal Interpretation</a></li>
  </ul></li>
  <li><a href="#data-processing-as-a-reproducible-workflow" id="toc-data-processing-as-a-reproducible-workflow" class="nav-link" data-scroll-target="#data-processing-as-a-reproducible-workflow"><span class="header-section-number">9</span> Data Processing as a Reproducible Workflow</a>
  <ul class="collapse">
  <li><a href="#why-reproducibility-matters" id="toc-why-reproducibility-matters" class="nav-link" data-scroll-target="#why-reproducibility-matters"><span class="header-section-number">9.1</span> Why Reproducibility Matters</a></li>
  <li><a href="#scripting-versus-manual-processing" id="toc-scripting-versus-manual-processing" class="nav-link" data-scroll-target="#scripting-versus-manual-processing"><span class="header-section-number">9.2</span> Scripting Versus Manual Processing</a></li>
  <li><a href="#processing-with-the-future-in-mind" id="toc-processing-with-the-future-in-mind" class="nav-link" data-scroll-target="#processing-with-the-future-in-mind"><span class="header-section-number">9.3</span> Processing With the Future in Mind</a></li>
  <li><a href="#documentation-as-part-of-the-workflow" id="toc-documentation-as-part-of-the-workflow" class="nav-link" data-scroll-target="#documentation-as-part-of-the-workflow"><span class="header-section-number">9.4</span> Documentation as Part of the Workflow</a></li>
  <li><a href="#connecting-workflow-to-this-course" id="toc-connecting-workflow-to-this-course" class="nav-link" data-scroll-target="#connecting-workflow-to-this-course"><span class="header-section-number">9.5</span> Connecting Workflow to This Course</a></li>
  </ul></li>
  <li><a href="#check-your-understanding" id="toc-check-your-understanding" class="nav-link" data-scroll-target="#check-your-understanding"><span class="header-section-number">10</span> Check Your Understanding</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">11</span> References</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Data Processing Reader</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>This document covers key concepts of data processing, drawing on applied data science resources. It emphasizes why data processing matters, the importance of understanding data context, and common components of data processing workflows.</p>
<div id="reading-time-placeholder">

</div>
<hr>
<section id="why-data-processing-matters" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="why-data-processing-matters"><span class="header-section-number">1</span> Why Data Processing Matters</h2>
<p>Before building models, creating visualizations, or running statistical tests, analysts must confront a more fundamental task: <strong>making sure the data are suitable for the question being asked</strong>. This step, often called <em>data processing</em>, <em>data cleaning</em>, or <em>data wrangling</em>, is where raw information is transformed into data suitable for analysis.</p>
<p>Although data processing often receives less attention than modeling or prediction, many authors emphasize that it is one of the most consequential stages of data analysis. Poorly processed data can undermine even the most sophisticated analytical techniques, while careful processing can substantially improve the reliability and interpretability of results <span class="citation" data-cites="datacamp_cleaning openstax_ds">(<a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>; <a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
<section id="garbage-in-garbage-out" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="garbage-in-garbage-out"><span class="header-section-number">1.1</span> “Garbage In, Garbage Out”</h3>
<p>A common phrase in data analysis is <em>“garbage in, garbage out.”</em> The idea is simple: if the input data are flawed, inconsistent, or misaligned with the problem of interest, then the outputs of any analysis will also be flawed. No amount of statistical sophistication can compensate for fundamental problems in the data itself <span class="citation" data-cites="datacamp_cleaning">(<a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>)</span>.</p>
<p>This is not a critique of modern modeling tools, machine learning algorithms, or AI systems. Rather, it reflects a basic constraint: <strong>analytical methods amplify whatever structure exists in the processed dataset</strong>. If that structure reflects measurement error, inconsistent definitions, or inappropriate transformations, the resulting conclusions may be precise but misleading <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
<p>In applied settings, the consequences are not merely academic. In business and policy contexts, poor data preparation can lead to:</p>
<ul>
<li>Overconfident forecasts based on biased inputs</li>
<li>Misallocation of resources toward the wrong populations or regions</li>
<li>Failure to detect risks that matter most for decision-making</li>
</ul>
<p>From this perspective, data processing is not a preliminary chore. It is a form of quality control for evidence-based decisions.</p>
</section>
<section id="data-are-not-neutral-objects" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="data-are-not-neutral-objects"><span class="header-section-number">1.2</span> Data Are Not Neutral Objects</h3>
<p>It is tempting to think of data as objective facts about the world, waiting to be analyzed. However, a consistent theme across applied data science and empirical research is that <strong>data are constructed objects</strong>, shaped by human, institutional, and technological choices <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
<p>What gets measured, how often it is measured, and at what level of aggregation all reflect constraints and incentives present at the time of data collection. As a result, datasets inevitably encode assumptions about what matters, what is observable, and what can be ignored.</p>
<p>For example, many real-world datasets are collected for administrative, commercial, or operational purposes rather than for research or policy analysis. This means that:</p>
<ul>
<li>Some populations may be underrepresented</li>
<li>Some outcomes may be measured indirectly rather than directly</li>
<li>Missing values may reflect reporting rules or suppression rather than absence</li>
</ul>
<p>Recognizing these features does not make data unusable. Instead, it allows analysts to interpret the data more accurately and process them in ways that are consistent with their limitations <span class="citation" data-cites="dime_handbook openstax_ds">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>; <a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
</section>
<section id="why-context-comes-before-cleaning" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="why-context-comes-before-cleaning"><span class="header-section-number">1.3</span> Why Context Comes Before Cleaning</h3>
<p>Because data reflect how they were generated, effective processing begins with understanding context. Analysts must ask:</p>
<ul>
<li>Why was this data collected?</li>
<li>For whom was it intended?</li>
<li>Using what definitions, technologies, and assumptions?</li>
</ul>
<p>Without this contextual understanding, common processing steps—such as dropping observations, aggregating values, or flagging outliers—can inadvertently distort the signal of interest. For instance, missing values are often treated as a technical inconvenience, but the reasons data are missing can be substantively meaningful. Missingness may reflect nonresponse, confidentiality protections, or systematic gaps in coverage rather than random error <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
<p>Similarly, outliers are frequently removed during cleaning, yet extreme values may represent exactly the events of greatest interest in applied work, such as droughts, price spikes, or rare but costly disruptions <span class="citation" data-cites="datacamp_cleaning">(<a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>)</span>.</p>
<p><strong>Thus, data processing is not primarily about making data look neat. It is about ensuring that transformations preserve the connection between the dataset and the real-world phenomenon it represents.</strong></p>
</section>
<section id="data-processing-as-an-analytical-decision" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="data-processing-as-an-analytical-decision"><span class="header-section-number">1.4</span> Data Processing as an Analytical Decision</h3>
<p>Every data processing choice embeds assumptions:</p>
<ul>
<li>About what variation matters</li>
<li>About what can be safely ignored</li>
<li>About the appropriate unit of observation and level of aggregation</li>
</ul>
<p>These assumptions directly influence conclusions. Two analysts starting from the same raw dataset may arrive at different results, not because one made a coding error, but because they made different—often implicit—processing decisions <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
<p>For this reason, data processing deserves the same level of care, documentation, and transparency as modeling. It is the stage where messy, real-world information is translated into structured inputs for analysis. When done thoughtfully, it increases credibility and trust. When done automatically or without reflection, it undermines both.</p>
<blockquote class="blockquote">
<p><strong>Key takeaway:</strong> Data processing matters because it determines whether analysis answers the question we care about—or a distorted version of it.</p>
</blockquote>
</section>
</section>
<section id="understanding-data-in-context" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="understanding-data-in-context"><span class="header-section-number">2</span> Understanding Data in Context</h2>
<p>Before any data are cleaned, reshaped, or summarized, analysts must understand <strong>where the data come from and what they represent</strong>. Data processing choices are only meaningful when they are informed by how the data were generated. Without this context, even well-intentioned cleaning steps can introduce bias or erase important information.</p>
<p>Several data science and applied research texts emphasize that understanding data provenance is a prerequisite for credible analysis, not an optional background detail <span class="citation" data-cites="openstax_ds dime_handbook">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>; <a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
<section id="how-data-are-collected-shapes-what-they-measure" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="how-data-are-collected-shapes-what-they-measure"><span class="header-section-number">2.1</span> How Data Are Collected Shapes What They Measure</h3>
<p>All datasets reflect a sequence of design decisions made before analysis begins. These include decisions about:</p>
<ul>
<li>What is measured</li>
<li>How often measurements occur</li>
<li>Who or what is included</li>
<li>What level of spatial or temporal detail is recorded</li>
</ul>
<p>Many real-world datasets are not collected to answer research or policy questions directly. Instead, they are produced for administrative, operational, or commercial purposes. As a result, analysts inherit data that may only imperfectly align with their questions of interest <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
<p>For example, agricultural production data may be designed for reporting totals rather than capturing variability. Business transaction data may reflect customers who choose to participate rather than the full population. Mobility data may prioritize coverage and scalability over representativeness. These features do not make the data unusable, but they impose limits that must be acknowledged during processing.</p>
</section>
<section id="example-mobile-device-location-data" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="example-mobile-device-location-data"><span class="header-section-number">2.2</span> Example: Mobile Device Location Data</h3>
<p>Mobile device location data provide a useful illustration of why context matters. These data are widely used to study economic activity, human mobility, and responses to policy or environmental shocks. However, the numbers observed in these datasets are not direct measurements of people or behavior.</p>
<p>At a high level, mobile location data are generated when devices emit location signals through applications or operating systems. Data providers then process these raw signals to infer movement patterns and visits to specific locations. A common construct in such datasets is a “visit” to a point of interest, which is typically defined when a device enters a predefined geographic boundary, known as a geofence, and remains there for at least a minimum duration.</p>
<p>Several layers of processing occur before an analyst ever sees the data:</p>
<ul>
<li>Devices are sampled, not universally observed</li>
<li>Locations are inferred rather than directly measured</li>
<li>Visits are constructed using vendor-defined rules</li>
</ul>
<p>As a result, a reported count of visits does not represent the true number of people who entered a location. It represents the number of devices that met a particular set of criteria defined by the data provider. Use of these data therefore requires understanding these construction rules and their implications for analysis. Who does the data represent? What behaviors are captured or missed? How do these features vary across time and space?</p>
</section>
<section id="implications-for-data-processing" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="implications-for-data-processing"><span class="header-section-number">2.3</span> Implications for Data Processing</h3>
<p>Understanding how mobile location data are constructed has direct implications for how they should be processed. For example:</p>
<ul>
<li>Devices are not people. Some individuals carry multiple devices, while others carry none.</li>
<li>Coverage varies systematically by age, income, and geography.</li>
<li>Short visits or informal activity may be undercounted if they fall below duration thresholds.</li>
</ul>
<p>If these features are ignored, routine processing steps such as aggregation, normalization, or outlier detection can produce misleading results. For instance, changes in visit counts over time may reflect changes in device coverage rather than changes in behavior. Similarly, apparent differences across locations may arise from differences in sampling intensity rather than true differences in activity.</p>
<p>Texts on applied data analysis emphasize that analysts should treat constructed measures with caution and process them in ways that respect their underlying assumptions <span class="citation" data-cites="openstax_ds datacamp_cleaning">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>; <a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>)</span>.</p>
</section>
<section id="context-informs-what-is-reasonable-to-change" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="context-informs-what-is-reasonable-to-change"><span class="header-section-number">2.4</span> Context Informs What Is Reasonable to Change</h3>
<p>Context does not tell analysts exactly how to process data, but it constrains what choices are defensible. Knowing how data were collected helps answer questions such as:</p>
<ul>
<li>Is it appropriate to drop missing observations, or does missingness convey information?</li>
<li>Should values be aggregated, or does aggregation obscure meaningful variation?</li>
<li>Are extreme values likely to be errors, or are they plausible outcomes?</li>
</ul>
<p>In the case of mobile device data, removing outliers might eliminate errors, but it might also remove true responses to rare events such as natural disasters or major policy interventions. Similarly, aggregating visits to a monthly level may reduce noise, but it may also mask short-term behavioral responses that are central to the analysis.</p>
<p>For these reasons, data processing should be understood as a context-dependent activity rather than a mechanical checklist <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
</section>
<section id="from-context-to-processing-decisions" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="from-context-to-processing-decisions"><span class="header-section-number">2.5</span> From Context to Processing Decisions</h3>
<p>The goal of understanding data context is not to discourage analysis, but to guide it. Analysts who understand how data are generated are better equipped to:</p>
<ul>
<li>Choose appropriate units of observation</li>
<li>Decide which transformations preserve meaning</li>
<li>Communicate limitations transparently to decision-makers</li>
</ul>
<p>In applied data science, credibility often depends less on methodological complexity and more on whether the analyst can clearly explain what the data represent and how processing choices affect interpretation. Context is the foundation for that explanation.</p>
<blockquote class="blockquote">
<p><strong>Key takeaway:</strong> Understanding how data are collected and constructed is essential for making defensible data processing decisions.</p>
</blockquote>
</section>
</section>
<section id="what-do-we-mean-by-data-processing" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="what-do-we-mean-by-data-processing"><span class="header-section-number">3</span> What Do We Mean by Data Processing?</h2>
<p>The terms <em>data processing</em>, <em>data cleaning</em>, <em>data wrangling</em>, and <em>data preprocessing</em> are often used interchangeably. In practice, they refer to overlapping sets of activities that transform raw data into a form suitable for analysis. While different fields emphasize different aspects, the core idea is the same: <strong>data processing is the work required to align data with an analytical purpose</strong> <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
<p>Rather than a single step, data processing is a collection of decisions that determine what information enters an analysis and in what form.</p>
<section id="beyond-cleaning" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="beyond-cleaning"><span class="header-section-number">3.1</span> Beyond “Cleaning”</h3>
<p>In everyday language, data processing is often described as “cleaning,” which can give the misleading impression that the goal is simply to remove errors or make data look neat. In reality, many datasets are not dirty in an absolute sense. They are incomplete, inconsistent, or awkward because they were collected for a different purpose than the one the analyst now has in mind.</p>
<p>Applied data science texts emphasize that processing involves much more than error correction. It includes reshaping data, defining units of observation, creating variables, and deciding how to handle ambiguity and uncertainty <span class="citation" data-cites="openstax_ds datacamp_cleaning">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>; <a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>)</span>.</p>
<p>For example:</p>
<ul>
<li>A dataset with multiple observations per entity is not incorrect, but it may need to be aggregated.</li>
<li>Categorical variables coded numerically are not errors, but they require interpretation.</li>
<li>Missing values are not necessarily problems to be fixed, but signals to be understood.</li>
</ul>
<p>Seen this way, data processing is not about forcing data into a generic “clean” state. It is about making them usable for a specific analytical task.</p>
</section>
<section id="alignment-with-the-question" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="alignment-with-the-question"><span class="header-section-number">3.2</span> Alignment With the Question</h3>
<p>A central theme of this course is that good analysis begins with a well-defined question. Data processing is the stage where that question begins to shape the dataset itself.</p>
<p>Processing choices determine:</p>
<ul>
<li>What counts as an observation</li>
<li>Which variables are included</li>
<li>What level of aggregation is used</li>
<li>How variation is represented</li>
</ul>
<p>For instance, a policy question about long-term trends may require aggregating noisy daily data into annual averages. A question about short-term behavioral responses may require preserving fine temporal detail. Neither choice is inherently correct or incorrect, but each aligns the data with a different question.</p>
<p>The DIME Analytics Data Handbook emphasizes that processing should be guided by the intended analysis rather than by convenience or convention <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>. Analysts who skip this alignment risk answering a different question than the one they intend to study.</p>
</section>
<section id="common-components-of-data-processing" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="common-components-of-data-processing"><span class="header-section-number">3.3</span> Common Components of Data Processing</h3>
<p>Although data processing is context-specific, several components appear frequently across applications <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>:</p>
<ul>
<li>Structuring data into a consistent, tidy format</li>
<li>Resolving inconsistent records and definitions</li>
<li>Identifying and handling duplicate observations</li>
<li>Interpreting and recoding variables</li>
<li>Addressing missing values and extreme observations</li>
<li>Aggregating or binning data to appropriate scales</li>
</ul>
<p>These components are not independent. Decisions made in one area often constrain choices in others. For example, aggregation decisions affect how missing values and outliers appear, while encoding decisions affect how inconsistencies are detected.</p>
</section>
<section id="processing-choices-can-affect-results" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="processing-choices-can-affect-results"><span class="header-section-number">3.4</span> Processing Choices Can Affect Results</h3>
<p>One reason data processing deserves careful attention is that it is a major source of variation across analyses. Two analysts working with the same raw data can produce different processed datasets based on reasonable but distinct assumptions. These differences can lead to different estimates, different visual patterns, and different conclusions <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
<p>This variability is not necessarily a problem. It reflects the fact that data processing involves judgment. However, it does mean that processing choices should be explicit, documented, and justified. Treating processing as a purely mechanical step obscures its role in shaping results.</p>
</section>
<section id="a-working-definition" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="a-working-definition"><span class="header-section-number">3.5</span> A Working Definition</h3>
<p>For the purposes of this course, we use the following working definition:</p>
<p><strong>Data processing is the set of decisions and transformations that convert raw data into a structured dataset that can credibly answer a specific question.</strong></p>
<p>This definition emphasizes three ideas:</p>
<ul>
<li>Processing involves decisions, not just technical steps.</li>
<li>The goal is credibility, not cosmetic cleanliness.</li>
<li>The question, not the dataset, determines what processing is appropriate.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Key takeaway:</strong> Data processing is the bridge between raw data and analysis. It determines what information enters the analysis and how it is interpreted.</p>
</blockquote>
</section>
</section>
<section id="making-data-tidy" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="making-data-tidy"><span class="header-section-number">4</span> Making Data Tidy</h2>
<p>One of the most important and widely applicable goals of data processing is making data <em>tidy</em>. Tidy data provide a consistent structure that makes analysis, visualization, and modeling easier and less error-prone. Many data processing problems are not about incorrect values, but about inconvenient or ambiguous structure.</p>
<p>The concept of tidy data is emphasized across data science and applied research texts because it provides a common foundation for downstream analysis <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
<section id="what-does-tidy-mean" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="what-does-tidy-mean"><span class="header-section-number">4.1</span> What Does “Tidy” Mean?</h3>
<p>A dataset is considered tidy when it satisfies three basic principles:</p>
<ul>
<li>Each variable forms its own column</li>
<li>Each observation forms its own row</li>
<li>Each type of observational unit forms its own table</li>
</ul>
<p>These principles may seem abstract, but they have very practical implications. When data are tidy, common operations such as filtering, grouping, summarizing, and plotting become more transparent and less error-prone.</p>
<p>Untidy data, by contrast, often require special-case handling, encourage ad hoc fixes, and make it harder to see whether results reflect real patterns or artifacts of structure <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
</section>
<section id="a-common-untidy-example" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="a-common-untidy-example"><span class="header-section-number">4.2</span> A Common Untidy Example</h3>
<p>Consider a dataset reporting agricultural outcomes by county and year. In a raw or lightly processed file, yields for different crops might be stored in separate columns:</p>
<ul>
<li>county</li>
<li>year</li>
<li>corn_yield</li>
<li>wheat_yield</li>
<li>soy_yield</li>
</ul>
<p>At first glance, this format appears convenient. However, the crop type is embedded in the column names rather than represented explicitly as data. This can make it difficult to answer questions such as:</p>
<ul>
<li>How do yields compare across crops?</li>
<li>How does drought affect different crops differently?</li>
<li>How many observations are there per crop?</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>county</th>
<th style="text-align: right;">year</th>
<th style="text-align: right;">corn_yield</th>
<th style="text-align: right;">wheat_yield</th>
<th style="text-align: right;">soy_yield</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>County A</td>
<td style="text-align: right;">2020</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">80</td>
<td style="text-align: right;">45</td>
</tr>
<tr class="even">
<td>County B</td>
<td style="text-align: right;">2020</td>
<td style="text-align: right;">95</td>
<td style="text-align: right;">70</td>
<td style="text-align: right;">50</td>
</tr>
</tbody>
</table>
</section>
<section id="the-tidy-version-of-the-same-data" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="the-tidy-version-of-the-same-data"><span class="header-section-number">4.3</span> The Tidy Version of the Same Data</h3>
<p>A tidy version of the same dataset would restructure the data so that crop type appears as a column:</p>
<ul>
<li>county</li>
<li>year</li>
<li>crop</li>
<li>yield</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>county</th>
<th style="text-align: right;">year</th>
<th>crop</th>
<th style="text-align: right;">yield</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>County A</td>
<td style="text-align: right;">2020</td>
<td>corn</td>
<td style="text-align: right;">120</td>
</tr>
<tr class="even">
<td>County A</td>
<td style="text-align: right;">2020</td>
<td>wheat</td>
<td style="text-align: right;">80</td>
</tr>
<tr class="odd">
<td>County A</td>
<td style="text-align: right;">2020</td>
<td>soy</td>
<td style="text-align: right;">45</td>
</tr>
<tr class="even">
<td>County B</td>
<td style="text-align: right;">2020</td>
<td>corn</td>
<td style="text-align: right;">95</td>
</tr>
<tr class="odd">
<td>County B</td>
<td style="text-align: right;">2020</td>
<td>wheat</td>
<td style="text-align: right;">70</td>
</tr>
<tr class="even">
<td>County B</td>
<td style="text-align: right;">2020</td>
<td>soy</td>
<td style="text-align: right;">50</td>
</tr>
</tbody>
</table>
<p>Each row now represents a single observation of yield for a specific crop, county, and year. Although this format often results in more rows, it has several advantages:</p>
<ul>
<li>Crop type can be filtered, grouped, or compared directly</li>
<li>Summary statistics by crop are straightforward</li>
<li>Visualization and modeling workflows are simpler and more consistent</li>
</ul>
<p>Importantly, this transformation does not change the underlying information. It changes how that information is represented so that it aligns with analytical tasks <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
</section>
<section id="when-structure-hides-meaning" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="when-structure-hides-meaning"><span class="header-section-number">4.4</span> When Structure Hides Meaning</h3>
<p>Untidy structure often hides meaning in subtle ways. Another common example involves time stored across multiple columns. A dataset might record monthly values like this:</p>
<ul>
<li>region</li>
<li>year</li>
<li>jan</li>
<li>feb</li>
<li>mar</li>
<li>apr</li>
</ul>
<p>In this format, month is a variable, but it is encoded as column names rather than values. This makes it difficult to:</p>
<ul>
<li>Plot trends over time</li>
<li>Merge with other time-based datasets</li>
<li>Apply consistent transformations across months</li>
</ul>
<p>A tidy version would instead include:</p>
<ul>
<li>region</li>
<li>year</li>
<li>month</li>
<li>value</li>
</ul>
<p>This structure makes the temporal dimension explicit and allows the analyst to treat time as data rather than metadata.</p>
</section>
<section id="why-tidy-data-matter-for-decision-making" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="why-tidy-data-matter-for-decision-making"><span class="header-section-number">4.5</span> Why Tidy Data Matter for Decision-Making</h3>
<p>Tidy data are not an aesthetic preference. They directly affect the reliability and transparency of analysis. When variables are stored consistently:</p>
<ul>
<li>Processing steps can be scripted and reused; especially important when data sets get large</li>
<li>Errors are easier to detect</li>
<li>Assumptions are easier to communicate</li>
</ul>
<p>Applied data science guides emphasize that tidy structure reduces the need for one-off fixes and makes analytical workflows more reproducible <span class="citation" data-cites="datacamp_cleaning openstax_ds">(<a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>; <a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>. This is especially important when data are updated regularly or when multiple analysts work with the same datasets.</p>
</section>
<section id="tidy-data-are-not-the-end-point" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="tidy-data-are-not-the-end-point"><span class="header-section-number">4.6</span> Tidy Data Are Not the End Point</h3>
<p>It is important to note that tidy data are not necessarily final data. They are a foundation. Additional processing may still be required to:</p>
<ul>
<li>Resolve inconsistencies</li>
<li>Handle missing values</li>
<li>Aggregate observations</li>
<li>Create derived variables</li>
</ul>
<p>However, working with tidy data makes these tasks more systematic and less error-prone. For this reason, many data processing workflows treat tidying as an early and essential step.</p>
<blockquote class="blockquote">
<p><strong>Key takeaway:</strong> Making data tidy means making variables explicit and structure consistent. This simplifies analysis and reduces the risk that results are driven by hidden structural choices rather than real patterns.</p>
</blockquote>
</section>
</section>
<section id="inconsistent-records-and-duplicates" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="inconsistent-records-and-duplicates"><span class="header-section-number">5</span> Inconsistent Records and Duplicates</h2>
<p>Once data are structured in a tidy format, a common next challenge is dealing with <strong>inconsistent records and duplicate observations</strong>. These issues are widespread in real-world datasets and can quietly distort results if left unaddressed.</p>
<p>Inconsistencies and duplicates often arise not from mistakes by analysts, but from the realities of data collection across time, institutions, and systems. Recognizing and resolving them is a core component of data processing <span class="citation" data-cites="openstax_ds dime_handbook">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>; <a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
<section id="inconsistent-records" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="inconsistent-records"><span class="header-section-number">5.1</span> Inconsistent Records</h3>
<p>Inconsistent records occur when the same concept is represented in multiple ways within a dataset. These inconsistencies can appear in variable names, category labels, units of measurement, or definitions that change over time.</p>
<p>Common examples include:</p>
<ul>
<li>Different spellings or abbreviations for the same location or entity</li>
<li>Categories that differ only in capitalization or formatting</li>
<li>Numeric values recorded in different units</li>
<li>Definitions that shift across reporting periods</li>
</ul>
<p>For instance, a dataset may record irrigation status as “Yes,” “Y,” and “1” in different rows. Although these values refer to the same underlying concept, they will be treated as distinct categories unless explicitly reconciled. This can lead to misleading summaries, incorrect counts, or fragmented groups during analysis.</p>
</section>
<section id="why-inconsistencies-matter" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="why-inconsistencies-matter"><span class="header-section-number">5.2</span> Why Inconsistencies Matter</h3>
<p>Inconsistencies rarely cause errors that are obvious at first glance. Instead, they create subtle problems:</p>
<ul>
<li>Grouped summaries may split what should be a single category</li>
<li>Visualizations may show duplicate labels that appear meaningful but are not</li>
<li>Models may treat equivalent values as separate predictors</li>
</ul>
<p>Because these problems do not always trigger warnings or errors, analysts must actively look for them. This often involves examining unique values, frequency tables, and summary statistics before proceeding with analysis <span class="citation" data-cites="datacamp_cleaning">(<a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>)</span>.</p>
</section>
<section id="duplicate-records" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="duplicate-records"><span class="header-section-number">5.3</span> Duplicate Records</h3>
<p>Duplicate records occur when the same observation appears more than once in a dataset. These duplicates may be exact copies, or they may differ slightly due to formatting or timing differences.</p>
<p>Duplicates arise for many reasons:</p>
<ul>
<li>Data collected from multiple sources are merged</li>
<li>Records are updated without removing prior versions</li>
<li>Systems log events multiple times</li>
<li>Observations are repeated unintentionally during data extraction</li>
</ul>
<p>Importantly, <strong>not all duplicates are errors</strong>. Some datasets are designed to include repeated observations, such as multiple transactions by the same customer or repeated measurements over time. The challenge is distinguishing between meaningful repetition and unintended duplication.</p>
<p>Unintended duplicates can bias results by overweighting certain observations. For example:</p>
<ul>
<li>A duplicated record may double-count an event</li>
<li>Repeated entries for certain entities may distort averages</li>
<li>Summary statistics may be driven by data artifacts rather than real patterns</li>
</ul>
</section>
<section id="resolving-inconsistencies-and-duplicates" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="resolving-inconsistencies-and-duplicates"><span class="header-section-number">5.4</span> Resolving Inconsistencies and Duplicates</h3>
<p>Addressing inconsistent records and duplicates typically involves:</p>
<ul>
<li>Standardizing labels and units</li>
<li>Defining clear rules for what constitutes a unique observation</li>
<li>Documenting assumptions used to resolve ambiguity</li>
</ul>
<p>These steps should be guided by the analytical question rather than by a desire to maximize uniformity. For example, collapsing categories may simplify analysis, but it may also remove distinctions that matter for interpretation.</p>
<p>As with other processing decisions, transparency is critical. Analysts should be able to explain how inconsistencies were resolved and why certain duplicates were retained or removed.</p>
<blockquote class="blockquote">
<p><strong>Key takeaway:</strong> Inconsistent records and duplicate observations are common features of real-world data. Resolving them requires understanding what each observation represents and making deliberate, documented processing choices.</p>
</blockquote>
</section>
</section>
<section id="encoding-and-variable-representation" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="encoding-and-variable-representation"><span class="header-section-number">6</span> Encoding and Variable Representation</h2>
<p>Many datasets represent information using codes, labels, or numeric placeholders rather than directly storing the concepts of interest. This practice, known as <em>encoding</em>, is common in administrative, business, and large-scale observational data. While encoding is often necessary for efficient storage or privacy, it can introduce ambiguity if the meaning of codes is not clearly understood.</p>
<p>Understanding how variables are encoded is a critical step in data processing because encoding choices shape how data are interpreted and analyzed <span class="citation" data-cites="openstax_ds dime_handbook">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>; <a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
<section id="why-encoding-exists" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="why-encoding-exists"><span class="header-section-number">6.1</span> Why Encoding Exists</h3>
<p>Variables are often encoded for practical reasons:</p>
<ul>
<li>To reduce file size</li>
<li>To comply with reporting standards</li>
<li>To anonymize sensitive information</li>
<li>To simplify data entry or transmission</li>
</ul>
<p>For example, a dataset may represent crop type as numeric codes rather than text labels, or use integers to represent categorical responses such as land use type or survey answers. These encodings are not inherently problematic, but they require careful interpretation.</p>
</section>
<section id="when-encoding-causes-problems" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="when-encoding-causes-problems"><span class="header-section-number">6.2</span> When Encoding Causes Problems</h3>
<p>Encoding becomes problematic when analysts treat codes as if they were meaningful numeric values or assume that categories are ordered when they are not.</p>
<p>Common pitfalls include:</p>
<ul>
<li>Treating categorical codes as continuous variables</li>
<li>Assuming numeric codes imply ranking or distance</li>
<li>Mixing encoded and unencoded values within the same variable</li>
</ul>
<p>For instance, a variable coded as 1 = corn, 2 = wheat, and 3 = soy does not imply that wheat lies between corn and soy in any meaningful sense. If this variable is treated as numeric in analysis, it can introduce artificial relationships that do not exist in reality <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
<p>Similarly, binary variables are often encoded using 0 and 1, but the meaning of each value is not always obvious. Without documentation, it may be unclear whether 1 indicates presence or absence, participation or non-participation, or whether missing values are coded separately.</p>
</section>
<section id="inconsistent-or-ambiguous-encoding" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="inconsistent-or-ambiguous-encoding"><span class="header-section-number">6.3</span> Inconsistent or Ambiguous Encoding</h3>
<p>Encoding problems are compounded when the same concept is encoded differently across time, sources, or variables. For example:</p>
<ul>
<li>A missing value may be coded as -99 in one year and left blank in another</li>
<li>A categorical variable may use text labels in some records and numeric codes in others</li>
<li>Binary variables may use different conventions across datasets</li>
</ul>
<p>These inconsistencies can produce misleading results if they are not resolved prior to analysis. Automated tools will treat distinct codes as distinct values, even when they refer to the same underlying concept <span class="citation" data-cites="datacamp_cleaning">(<a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>)</span>.</p>
</section>
<section id="encoding-and-interpretation" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="encoding-and-interpretation"><span class="header-section-number">6.4</span> Encoding and Interpretation</h3>
<p>Encoding choices influence how analysts and decision-makers interpret results. A variable labeled with clear, descriptive categories is easier to understand and communicate than one labeled with abstract codes. For this reason, many applied data science guides recommend recoding variables into human-readable formats early in the processing workflow <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
<p>This does not mean that numeric encodings should always be removed. In many cases, maintaining both a coded version and a labeled version of a variable can be useful, particularly when working with large datasets or multiple software tools. In some cases you may see companion lookup tables to map the codes into human-readable labels. What matters is that the meaning of each encoding is explicit and documented.</p>
</section>
<section id="encoding-as-a-processing-decision" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="encoding-as-a-processing-decision"><span class="header-section-number">6.5</span> Encoding as a Processing Decision</h3>
<p>Like other aspects of data processing, encoding is not purely technical. Decisions about how variables are represented reflect assumptions about:</p>
<ul>
<li>Which distinctions matter</li>
<li>How variables will be used in analysis</li>
<li>How results will be communicated</li>
</ul>
<p>Poorly chosen or undocumented encodings can obscure meaning, while thoughtful representation can clarify patterns and support more transparent analysis <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
<blockquote class="blockquote">
<p><strong>Key takeaway:</strong> Encoding determines how variables are interpreted. Understanding and, when necessary, revising variable representation is essential for producing meaningful and communicable results.</p>
</blockquote>
</section>
</section>
<section id="handling-missing-data-and-outliers" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="handling-missing-data-and-outliers"><span class="header-section-number">7</span> Handling Missing Data and Outliers</h2>
<p>Missing values and extreme observations are among the most common features of real-world datasets. How these values are handled can have a substantial impact on analysis results, yet there is rarely a single correct approach. Instead, analysts must make context-dependent decisions that balance statistical convenience with substantive meaning.</p>
<p>Applied data science texts emphasize that missing data and outliers should be understood before they are modified or removed, since both often contain important information about the data-generating process <span class="citation" data-cites="openstax_ds datacamp_cleaning">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>; <a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>)</span>.</p>
<section id="why-data-are-missing" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="why-data-are-missing"><span class="header-section-number">7.1</span> Why Data Are Missing</h3>
<p>Data can be missing for many reasons, and these reasons matter for interpretation. Common causes include:</p>
<ul>
<li>Nonresponse or incomplete reporting</li>
<li>Measurement failure or data corruption</li>
<li>Suppression for confidentiality or privacy</li>
<li>Data that were never intended to be collected</li>
</ul>
<p>In many applied datasets, missing values are not random. For example, smaller producers may be more likely to have suppressed values, or certain populations may be systematically underrepresented. Treating all missing values as interchangeable can therefore introduce bias <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
<p>A critical first step in processing is determining whether missing values represent absence, unavailability, or intentional suppression.</p>
</section>
<section id="common-responses-to-missing-data" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="common-responses-to-missing-data"><span class="header-section-number">7.2</span> Common Responses to Missing Data</h3>
<p>There are several broad strategies for handling missing data, each with tradeoffs:</p>
<ul>
<li>Dropping observations with missing values</li>
<li>Flagging missingness as its own category</li>
<li>Imputing values using simple rules or models</li>
</ul>
<p>Dropping observations is often the easiest approach, but it can change the composition of the dataset in ways that are difficult to detect. Flagging missingness preserves information about where data are incomplete, which can be valuable in both descriptive and inferential contexts. Imputation can reduce data loss but introduces additional assumptions that should be made explicit <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
<p>The appropriate choice depends on why data are missing and how the variable is used in analysis.</p>
</section>
<section id="understanding-outliers" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="understanding-outliers"><span class="header-section-number">7.3</span> Understanding Outliers</h3>
<p>Outliers are observations that differ substantially from most other values in the dataset. They may arise from:</p>
<ul>
<li>Data entry or measurement error</li>
<li>Differences in scale or units</li>
<li>True but rare events</li>
</ul>
<p>Outliers are often treated as problems to be fixed, but in many applied contexts they are substantively meaningful. For example, extreme values may correspond to drought years, economic shocks, or policy interventions. Automatically removing them can eliminate the very variation of interest.</p>
<p>Data science guides caution against treating outliers as errors without investigating their origin <span class="citation" data-cites="datacamp_cleaning">(<a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>)</span>.</p>
</section>
<section id="outliers-and-analytical-goals" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="outliers-and-analytical-goals"><span class="header-section-number">7.4</span> Outliers and Analytical Goals</h3>
<p>Whether an outlier should be retained, transformed, or excluded depends on the analytical question. If the goal is to understand typical behavior, extreme values may obscure patterns. If the goal is to understand risk, vulnerability, or rare events, those same values may be central.</p>
<p>This distinction highlights an important principle: outliers are defined relative to an analytical purpose, not solely by statistical rules <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
</section>
<section id="transparency-and-documentation" class="level3" data-number="7.5">
<h3 data-number="7.5" class="anchored" data-anchor-id="transparency-and-documentation"><span class="header-section-number">7.5</span> Transparency and Documentation</h3>
<p>Decisions about missing data and outliers should be transparent and reproducible. Analysts should be able to explain:</p>
<ul>
<li>How missing values were identified</li>
<li>Why certain observations were excluded or retained</li>
<li>How these choices affect interpretation</li>
</ul>
<p>The DIME Analytics Data Handbook emphasizes that documenting these decisions is essential for credibility, particularly in policy and applied research settings where results may inform high-stakes decisions <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
<blockquote class="blockquote">
<p><strong>Key takeaway:</strong> Missing data and outliers are not just technical nuisances. They reflect how data were generated and often carry important information. Handling them requires judgment, context, and transparency.</p>
</blockquote>
</section>
</section>
<section id="aggregating-and-binning" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="aggregating-and-binning"><span class="header-section-number">8</span> Aggregating and Binning</h2>
<p>Many analyses require transforming detailed data into coarser summaries. This process, often referred to as <em>aggregation</em> or <em>binning</em>, is a central component of data processing. Aggregation combines multiple observations into summary measures, while binning groups continuous values into discrete categories. Both techniques can make patterns easier to see and align data with decision-making needs.</p>
<p>However, aggregation and binning also involve tradeoffs. They simplify data by design, and in doing so, they can obscure important variation if applied without care <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
<section id="why-we-aggregate-and-bin-data" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="why-we-aggregate-and-bin-data"><span class="header-section-number">8.1</span> Why We Aggregate and Bin Data</h3>
<p>There are several common reasons to aggregate or bin data:</p>
<ul>
<li>To match the scale of the decision being made</li>
<li>To reduce noise in highly variable data</li>
<li>To protect confidentiality</li>
<li>To improve interpretability and communication</li>
</ul>
<p>For example, daily observations may be aggregated to monthly or annual averages when analyzing long-term trends. Individual-level data may be aggregated to neighborhoods, counties, or regions when policies are implemented at those levels. Continuous variables such as income or yield may be binned into categories to facilitate comparison across groups.</p>
<p>In each case, aggregation helps align the data with the analytical or policy context <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
</section>
<section id="what-is-lost-through-aggregation" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="what-is-lost-through-aggregation"><span class="header-section-number">8.2</span> What Is Lost Through Aggregation</h3>
<p>While aggregation can clarify broad patterns, it necessarily removes detail. This loss of detail can have important consequences:</p>
<ul>
<li>Heterogeneity within groups is hidden</li>
<li>Extreme values may be averaged away</li>
<li>Subpopulations may be masked by group-level summaries</li>
</ul>
<p>For instance, aggregating agricultural yields at the state level may conceal large differences across counties or farms. Similarly, binning income into broad categories may hide important variation within bins that matters for equity or targeting policies.</p>
<p>Applied research texts caution that analysts should consider whether aggregation changes the question being answered, not just the appearance of the data <span class="citation" data-cites="openstax_ds">(<a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
</section>
<section id="binning-and-arbitrary-boundaries" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="binning-and-arbitrary-boundaries"><span class="header-section-number">8.3</span> Binning and Arbitrary Boundaries</h3>
<p>Binning introduces an additional challenge: the choice of cutoffs. The boundaries used to define bins are often arbitrary, yet they can strongly influence interpretation.</p>
<p>For example, grouping drought severity into categories such as low, medium, and high requires deciding where thresholds lie. Small changes in these thresholds can shift observations between bins, potentially altering conclusions.</p>
<p>Because of this sensitivity, binning decisions should be justified and, when possible, tested for robustness <span class="citation" data-cites="datacamp_cleaning">(<a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>)</span>.</p>
</section>
<section id="aggregation-and-causal-interpretation" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="aggregation-and-causal-interpretation"><span class="header-section-number">8.4</span> Aggregation and Causal Interpretation</h3>
<p>Aggregation also affects how results can be interpreted. Relationships observed in aggregated data may differ from relationships at finer scales. In some cases, aggregation can even reverse apparent patterns.</p>
<p>For this reason, analysts should be cautious when drawing conclusions from aggregated data, particularly when those conclusions are intended to inform decisions affecting individuals or subgroups <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
<p>There is no universally correct level of aggregation. Instead, analysts should choose levels that preserve relevant variation while reducing unnecessary noise.</p>
<blockquote class="blockquote">
<p><strong>Key takeaway:</strong> Aggregation and binning help align data with analytical and decision-making contexts, but they also remove detail. These choices should be made deliberately and with attention to what information is lost.</p>
</blockquote>
</section>
</section>
<section id="data-processing-as-a-reproducible-workflow" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="data-processing-as-a-reproducible-workflow"><span class="header-section-number">9</span> Data Processing as a Reproducible Workflow</h2>
<p>Up to this point, data processing has been described as a series of decisions about structure, representation, and interpretation. An equally important aspect of data processing is <strong>how those decisions are implemented</strong>. In applied data analysis, processing is most valuable when it can be repeated, reviewed, and extended. This is the motivation for treating data processing as a reproducible workflow rather than a one-time task.</p>
<p>Reproducibility is widely recognized as a core principle of credible data science and empirical research <span class="citation" data-cites="dime_handbook openstax_ds">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>; <a href="#ref-openstax_ds" role="doc-biblioref">OpenStax 2021</a>)</span>.</p>
<section id="why-reproducibility-matters" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="why-reproducibility-matters"><span class="header-section-number">9.1</span> Why Reproducibility Matters</h3>
<p>Many datasets are updated over time. New observations are added, reporting rules change, or additional regions become available. If data processing steps are not reproducible, analysts must repeat manual work each time the data change. This increases the risk of inconsistency and error.</p>
<p>Reproducible workflows allow analysts to:</p>
<ul>
<li>Apply the same processing steps to new data</li>
<li>Trace results back to raw inputs</li>
<li>Identify where assumptions enter the analysis</li>
<li>Share work with collaborators</li>
</ul>
<p>From a decision-making perspective, reproducibility supports accountability. When results are questioned, analysts can explain not only what the results are, but how they were produced <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
</section>
<section id="scripting-versus-manual-processing" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="scripting-versus-manual-processing"><span class="header-section-number">9.2</span> Scripting Versus Manual Processing</h3>
<p>A reproducible workflow typically relies on scripts rather than manual edits. Manual processing, such as editing spreadsheets by hand, can be quick for small tasks but is difficult to document and nearly impossible to reproduce exactly.</p>
<p>Scripting data processing steps has several advantages:</p>
<ul>
<li>Every transformation is recorded</li>
<li>Steps can be rerun in the same order</li>
<li>Changes can be tracked and reviewed</li>
<li>Processing logic can be reused across projects</li>
</ul>
<p>Data science guides emphasize that scripting does not require complex code. Even simple, clearly written scripts can dramatically improve transparency and reliability <span class="citation" data-cites="datacamp_cleaning">(<a href="#ref-datacamp_cleaning" role="doc-biblioref">DataCamp 2023</a>)</span>.</p>
</section>
<section id="processing-with-the-future-in-mind" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="processing-with-the-future-in-mind"><span class="header-section-number">9.3</span> Processing With the Future in Mind</h3>
<p>Reproducible workflows encourage analysts to think beyond a single assignment or report. Processing decisions should be made with future use cases in mind, such as:</p>
<ul>
<li>Updating analyses as new data arrive</li>
<li>Extending work to new locations or time periods</li>
<li>Sharing data and code with collaborators</li>
<li>Supporting audits or replication efforts</li>
</ul>
<p>This perspective shifts data processing from a short-term task to a form of analytical infrastructure.</p>
</section>
<section id="documentation-as-part-of-the-workflow" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="documentation-as-part-of-the-workflow"><span class="header-section-number">9.4</span> Documentation as Part of the Workflow</h3>
<p>Reproducibility is not achieved through code alone. Documentation plays a critical role in explaining why processing decisions were made and how they affect interpretation.</p>
<p>Good documentation includes:</p>
<ul>
<li>Clear descriptions of variables and units</li>
<li>Explanations of how missing data and outliers were handled</li>
<li>Justifications for aggregation and encoding choices</li>
</ul>
<p>The DIME Analytics Data Handbook emphasizes that well-documented processing steps are essential for credibility in applied and policy-oriented research, where results may influence high-stakes decisions <span class="citation" data-cites="dime_handbook">(<a href="#ref-dime_handbook" role="doc-biblioref">World Bank DIME Analytics 2020</a>)</span>.</p>
</section>
<section id="connecting-workflow-to-this-course" class="level3" data-number="9.5">
<h3 data-number="9.5" class="anchored" data-anchor-id="connecting-workflow-to-this-course"><span class="header-section-number">9.5</span> Connecting Workflow to This Course</h3>
<p>In this course, data processing is treated as a deliberate, transparent workflow. The goal is not to eliminate judgment, but to make it visible. By scripting and documenting processing steps, analysts make their assumptions explicit and allow others to evaluate whether the data are appropriate for the question being asked.</p>
<p>This approach prepares students for both academic research and applied work, where the ability to explain and defend data preparation choices is as important as producing results.</p>
<blockquote class="blockquote">
<p><strong>Key takeaway:</strong> Data processing is most effective when it is reproducible. Treating processing as a documented workflow improves transparency, reduces errors, and strengthens the credibility of analysis.</p>
</blockquote>
</section>
</section>
<section id="check-your-understanding" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="check-your-understanding"><span class="header-section-number">10</span> Check Your Understanding</h2>
<p>After completing the reading, answer the following questions. Your responses should be brief and reflect your understanding of the concepts, not technical details.</p>
<p>Understanding the Role of Data Processing</p>
<ol type="1">
<li><p>In one or two sentences, explain what is meant by “garbage in, garbage out” in the context of data analysis.</p></li>
<li><p>Describe one reason why data processing decisions can affect conclusions even before any model is estimated.</p></li>
<li><p>Give one example of how untidy data structure can make analysis more difficult.</p></li>
<li><p>Describe one potential problem caused by unclear or inconsistent variable encoding.</p></li>
<li><p>Explain one reason why missing data should not automatically be treated as zero or dropped.</p></li>
<li><p>Describe a situation in which an outlier might be important rather than an error.</p></li>
<li><p>In one sentence, explain why scripting data processing steps is preferable to manual edits.</p></li>
<li><p>Describe one benefit of documenting data processing decisions.</p></li>
<li><p>Which part of the data processing workflow do you expect to find most challenging, and why?</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>AI Statement: ChatGPT was the primary author of this document. I outlined the topics and structure, and provided specific content and references. I reviewed and edited the output to ensure accuracy and clarity. I take all responsibility for any errors or omissions.</p>
</div>
</div>
</section>
<section id="references" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="references"><span class="header-section-number">11</span> References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-datacamp_cleaning" class="csl-entry" role="listitem">
DataCamp. 2023. <span>“Data Cleaning Tutorial: How to Clean Data for Analysis.”</span> 2023. <a href="https://www.datacamp.com/tutorial/tutorial-data-cleaning-tutorial">https://www.datacamp.com/tutorial/tutorial-data-cleaning-tutorial</a>.
</div>
<div id="ref-openstax_ds" class="csl-entry" role="listitem">
OpenStax. 2021. <em>Principles of Data Science</em>. OpenStax. <a href="https://openstax.org/details/books/principles-data-science">https://openstax.org/details/books/principles-data-science</a>.
</div>
<div id="ref-dime_handbook" class="csl-entry" role="listitem">
World Bank DIME Analytics. 2020. <span>“The DIME Analytics Data Handbook.”</span> 2020. <a href="https://worldbank.github.io/dime-data-handbook/">https://worldbank.github.io/dime-data-handbook/</a>.
</div>
</div>
<script>
document.addEventListener("DOMContentLoaded", function(){
  const placeholder = document.getElementById('reading-time-placeholder');
  const text = (document.querySelector('main') || document.body).innerText || "";
  const words = text.trim().split(/\s+/).filter(Boolean).length || 0;
  const mins = Math.max(1, Math.round(words/200));
  const el = document.createElement('div');
  el.className = 'reading-time';
  el.textContent = 'Est read time: ' + mins + ' min' ;
  (placeholder || document.body).appendChild(el);
});
</script>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>